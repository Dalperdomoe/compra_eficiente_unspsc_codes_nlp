{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(987)\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = os.path.join(\"..\", \"data\", \"raw\", \"raw_data.csv\")\n",
    "df_raw = pd.read_csv(raw_file)\n",
    "\n",
    "df_sample = df_raw.sample(10000).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspsc_file = os.path.join(\"..\",\"references\",\"clasificador_de_bienes_y_servicios_v14_1.xlsx\")\n",
    "df_unspsc = pd.read_excel(unspsc_file)\n",
    "\n",
    "segment_dict = dict(\n",
    "    zip(\n",
    "        df_unspsc['Código Segmento'].astype('str'),\n",
    "        df_unspsc['Nombre Segmento']\n",
    "    )\n",
    ")\n",
    "family_dict = dict(\n",
    "    zip(\n",
    "        df_unspsc['Código Familia'].astype('str'),\n",
    "        df_unspsc['Nombre Familia']\n",
    "    )\n",
    ")\n",
    "class_dict = dict(\n",
    "    zip(\n",
    "        df_unspsc['Código Clase'].astype('str'),\n",
    "        df_unspsc['Nombre Clase']\n",
    "    )\n",
    ")\n",
    "commodity_dict = dict(\n",
    "    zip(\n",
    "        df_unspsc['Código Producto'].astype('str'),\n",
    "        df_unspsc['Nombre Producto']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['codigo_de_categoria_principal'] = df_sample['codigo_de_categoria_principal'].str.extract('([0-9]{8})', expand=False)\n",
    "\n",
    "df_sample['segment_code'] = df_sample['codigo_de_categoria_principal'].str[:2]\n",
    "df_sample['family_code'] = df_sample['codigo_de_categoria_principal'].str[:4]\n",
    "df_sample['class_code'] = df_sample['codigo_de_categoria_principal'].str[:6]\n",
    "df_sample['commodity_code'] = df_sample['codigo_de_categoria_principal'].str[:8]\n",
    "\n",
    "df_sample['segment_text'] = df_sample.segment_code.map(segment_dict)\n",
    "df_sample['family_text'] = df_sample.family_code.map(family_dict)\n",
    "df_sample['class_text'] = df_sample.class_code.map(class_dict)\n",
    "df_sample['commodity_text'] = df_sample.commodity_code.map(commodity_dict)\n",
    "\n",
    "df_sample['segment_code'] = df_sample['segment_code'].astype('Int64')\n",
    "df_sample['family_code'] = df_sample['family_code'].astype('Int64')\n",
    "df_sample['class_code'] = df_sample['class_code'].astype('Int64')\n",
    "df_sample['commodity_code'] = df_sample['commodity_code'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select sample with every class code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspsc_commodity_code = df_unspsc['Código Clase'].astype('Int64').dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sample[df_sample['class_code'].isin(unspsc_commodity_code)][['descripcion_del_proceso', 'class_code', 'class_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los tokenizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "# Descargamos las stopwords para espanhol\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "# Definimos la función de preprocesamiento\n",
    "def normalize_document(doc):\n",
    "    # Se convierten los téxtos a minúsculas\n",
    "    doc = doc.lower()\n",
    "    # Se eliminan caracteres especiales\n",
    "    doc = re.sub(r'[^a-zñàáâãäåèéêëìíîïòóôõöùúûüýÿ\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.strip()\n",
    "    # Tokenizado de documento\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # Eliminación de stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Retornamos una versión filtrada del texto\n",
    "    doc = ' '.join(tokens)\n",
    "    return doc\n",
    "# Vectorización de la función\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "df['Text'] = df['descripcion_del_proceso'].apply(normalize_document)\n",
    "\n",
    "df.drop(columns='descripcion_del_proceso', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df[\"class_code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {key:value for value, key in enumerate(df['class_code'].unique().tolist())}\n",
    "\n",
    "df['Labels'] = df[\"class_code\"].map(dict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df[\"Labels\"].values, num_classes=num_classes)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Text'], y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "  '''return BERT-like embeddings of input text\n",
    "  Args:\n",
    "    - sentences: list of strings\n",
    "  Output:\n",
    "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
    "  '''\n",
    "  preprocessed_text = preprocessor(sentences)\n",
    "  return encoder(preprocessed_text)['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "x = preprocessor(i)\n",
    "x = encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
    "\n",
    "model = tf.keras.Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "234/234 [==============================] - ETA: 0s - loss: 3.3226 - accuracy: 0.3739 - balanced_recall: 5.7201e-04 - balanced_precision: 0.0011 - balanced_f1_score: 7.0423e-04"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      balanced_recall,\n",
    "      balanced_precision,\n",
    "      balanced_f1_score\n",
    "]\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
    "                                                      patience = 3,\n",
    "                                                      restore_best_weights = True)\n",
    "\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)\n",
    "\n",
    "model_fit = model.fit(x_train, \n",
    "                      y_train, \n",
    "                      epochs = n_epochs,\n",
    "                      validation_data = (x_test, y_test),\n",
    "                      callbacks = [earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, n_epochs+1))\n",
    "metric_list = list(model_fit.history.keys())\n",
    "num_metrics = int(len(metric_list)/2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
    "\n",
    "for i in range(0, num_metrics):\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "  ax[i].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(reviews):\n",
    "  '''predict class of input text\n",
    "  Args:\n",
    "    - reviews (list of strings)\n",
    "  Output:\n",
    "    - class (list of int)\n",
    "  '''\n",
    "  return [np.argmax(pred) for pred in model.predict(reviews)]\n",
    "\n",
    "\n",
    "predict_class('Prestar con plena autonomía técnica y administrativa sus servicios profesionales en sistemas de la información  bibliotecología y archivística en el área de Gestión Documental  para apoyar la misión de la Biblioteca de la FUGA.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/text_classifier_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the model as needed for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# load model\n",
    "new_model = keras.models.load_model(\"/models/text_classifier_v1\")\n",
    "\n",
    "# test predictions\n",
    "[np.argmax(pred) for pred in new_model.predict('Prestación de Servicios Profesionales como Abogado (a) en la Subsecretaría de Acceso a Servicíos de Justicia  en desarrollo del proyecto denominado:  Fortalecimiento de los servicios de acceso a la justicia en Santiago de Cali  Según ficha EBI No. 26002080')]\n",
    "# output: [3, 1, 0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3f966273feecc803f70756ec54d7ac99871d4b8828d7b4125d8f9ec98086622"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
